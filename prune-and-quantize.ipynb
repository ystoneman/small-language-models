{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58d2ab92-e3a9-4adc-9a59-14030d02391c",
   "metadata": {},
   "source": [
    "# Small Language Model from LLM\n",
    "Download an LLM, prune and quantize it, and benchmark it each step of the way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d0bdf6-36ff-4dc3-acc5-d8197e941c1e",
   "metadata": {},
   "source": [
    "## Start by downloading an LLM\n",
    "I was going to use Llama 2 just because of how ubiquitous it currently is. However, I realized it requires HuggingFace authentication, because of how Meta AI has an approval process. To avoid cluttering the code with authentication, I just went with Mistral AI's Mistral model instead. We could choose larger versions of this model. However, to prove out and practice these model-optimization concepts, we can iterate faster with a smaller model like 7B.\n",
    "\n",
    "According to a discussion on HuggingFace, Llama-2 7B requires 28GB of GPU RAM. Assuming it is similar for Mistral 7B, and to be on the safe side, I'll over-provision with an ml.g5.4xlarge for my SageMaker Studio Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25d47b5-8cc1-4994-889d-c9ac5246d020",
   "metadata": {},
   "source": [
    "### Set up environment\n",
    "At first I got the error `KeyError: 'mistral'` when running `from_pretrained()`\n",
    "The solution was on [the model's HuggingFace page](https://huggingface.co/mistralai/Mistral-7B-v0.1#troubleshooting)\n",
    "\n",
    "I tried installing `evaluate` later in the script, right before using it, but that gave me a warning that a `transformers` process already started. Once I moved the `pip install evaluate` to here, that warning went away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e780782-bb76-4a90-b057-a892d69ae609",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade datasets evaluate sentence_transformers transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d6c0cc-b577-42eb-a439-dfa0c25c1366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cd4a07-2246-4198-85f6-04ecf5aa8b96",
   "metadata": {},
   "source": [
    "### Download LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956cb49e-eff3-488d-8446-f21ac4e81baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_repo = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_repo, torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41953666-36ca-42fd-b45e-911a2eb4dafa",
   "metadata": {},
   "source": [
    "### Verify LLM works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce12127-8eea-460f-be5e-f7f6915c8a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple prompt\n",
    "prompt = \"Write a Haiku explaining biodynamic farming.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57296e79-63c8-4ea8-be79-88d724a4e075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d697f808-17c1-433f-b0d9-146e1fb0225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate response\n",
    "output = model.generate(input_ids, max_length=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a4ee2-53e3-485e-a9f3-68e4cb8c8f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True) \n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c60e73-db99-4818-a793-185766fed5a6",
   "metadata": {},
   "source": [
    "## Benchmark FM for baseline\n",
    "Let's benchmark for accuracy, latency, and resource utilization (Memory, GPU, and CPU).\n",
    "\n",
    "### Set up environment for evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c48a70-637a-438a-aea9-0692e3ac422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load the pre-trained SAS model\n",
    "sas_model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "\n",
    "# Set the pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def compute_sas(predicted_answers, reference_answers):\n",
    "    \"\"\"\n",
    "    Compute the Semantic Answer Similarity (SAS) between a list of predicted answers and reference answers.\n",
    "    \n",
    "    Args:\n",
    "        predicted_answers (list of str): The list of predicted answer texts.\n",
    "        reference_answers (list of str): The list of reference answer texts.\n",
    "    \n",
    "    Returns:\n",
    "        float: The average SAS score between the predicted and reference answers.\n",
    "    \"\"\"\n",
    "    sas_scores = []\n",
    "    for predicted, reference in zip(predicted_answers, reference_answers):\n",
    "        predicted_embedding = sas_model.encode(predicted, convert_to_tensor=True)\n",
    "        reference_embedding = sas_model.encode(reference, convert_to_tensor=True)\n",
    "        sas_score = util.cos_sim(predicted_embedding, reference_embedding).item()\n",
    "        sas_scores.append(sas_score)\n",
    "    \n",
    "    return sum(sas_scores) / len(sas_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363df553-9982-41e6-893b-fb4c5a127263",
   "metadata": {},
   "source": [
    "### Benchmark for Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5360df-f0cc-4435-9692-ea094c94349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the evaluation metric and dataset\n",
    "metric = load(\"accuracy\")\n",
    "dataset = load_dataset(\"allenai/reward-bench\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678444ea-8ad0-49aa-96db-f10933ce52f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(examples):\n",
    "    predictions = []\n",
    "    \n",
    "    for prompt, chosen in zip(examples[\"prompt\"], examples[\"chosen\"]):\n",
    "        # Tokenize the input\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "        attention_mask = tokenizer(prompt, return_tensors=\"pt\").attention_mask.to(\"cuda\")\n",
    "        \n",
    "        # Generate the model's prediction\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            # max_new_tokens is more flexible than max_length,\n",
    "            # because it only caps the output\n",
    "            # so it won't fail if input is longer than the specified value\n",
    "            max_new_tokens=100,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        \n",
    "        # Decode the generated text\n",
    "        predicted_answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Append the predicted answer and reference answer to the predictions list\n",
    "        predictions.append({\"predicted\": predicted_answer, \"reference\": chosen})\n",
    "    \n",
    "    # Return the predictions\n",
    "    return {\"predictions\": predictions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef66b77-d87a-413a-b5d1-edff4805e614",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Evaluate the model on the dataset\n",
    "results = dataset.map(generate_predictions, batched=True, batch_size=32)\n",
    "predicted_answers = [pred[\"predicted\"] for pred in results[\"predictions\"]]\n",
    "reference_answers = [pred[\"reference\"] for pred in results[\"predictions\"]]\n",
    "sas_score = compute_sas(predicted_answers, reference_answers)\n",
    "print(f\"Semantic Answer Similarity: {sas_score:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deafd608-d235-4b97-8f85-c8842a0045d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
