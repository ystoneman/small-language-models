{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58d2ab92-e3a9-4adc-9a59-14030d02391c",
   "metadata": {},
   "source": [
    "# Small Language Model from LLM\n",
    "Download an LLM, prune and quantize it, and benchmark it each step of the way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d0bdf6-36ff-4dc3-acc5-d8197e941c1e",
   "metadata": {},
   "source": [
    "## Start by downloading an LLM\n",
    "I was going to use Llama 2 just because of how ubiquitous it currently is. However, I realized it requires HuggingFace authentication, because of how Meta AI has an approval process. To avoid cluttering the code with authentication, I just went with Mistral AI's Mistral model instead. We could choose larger versions of this model. However, to prove out and practice these model-optimization concepts, we can iterate faster with a smaller model like 7B.\n",
    "\n",
    "According to a discussion on HuggingFace, Llama-2 7B requires 28GB of GPU RAM. Assuming it is similar for Mistral 7B, and to be on the safe side, I'll over-provision with an ml.g5.4xlarge for my SageMaker Studio Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25d47b5-8cc1-4994-889d-c9ac5246d020",
   "metadata": {},
   "source": [
    "### Set up environment\n",
    "At first I got the error `KeyError: 'mistral'` when running `from_pretrained()`\n",
    "The solution was on [the model's HuggingFace page](https://huggingface.co/mistralai/Mistral-7B-v0.1#troubleshooting)\n",
    "\n",
    "I tried installing `evaluate` later in the script, right before using it, but that gave me a warning that a `transformers` process already started. Once I moved the `pip install evaluate` to here, that warning went away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e780782-bb76-4a90-b057-a892d69ae609",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade datasets evaluate sentence_transformers transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79d6c0cc-b577-42eb-a439-dfa0c25c1366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cd4a07-2246-4198-85f6-04ecf5aa8b96",
   "metadata": {},
   "source": [
    "### Download LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956cb49e-eff3-488d-8446-f21ac4e81baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_repo = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_repo, torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41953666-36ca-42fd-b45e-911a2eb4dafa",
   "metadata": {},
   "source": [
    "### Verify LLM works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ce12127-8eea-460f-be5e-f7f6915c8a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple prompt\n",
    "prompt = \"Write a Haiku explaining biodynamic farming.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57296e79-63c8-4ea8-be79-88d724a4e075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d697f808-17c1-433f-b0d9-146e1fb0225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate response\n",
    "output = model.generate(input_ids, max_length=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa7a4ee2-53e3-485e-a9f3-68e4cb8c8f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a Haiku explaining biodynamic farming.\n",
      "\n",
      "The moon is full\n",
      "\n",
      "The sun is shining bright\n",
      "\n",
      "The earth is fertile\n",
      "\n",
      "The moon\n"
     ]
    }
   ],
   "source": [
    "# Decode generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True) \n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c60e73-db99-4818-a793-185766fed5a6",
   "metadata": {},
   "source": [
    "## Benchmark FM for baseline\n",
    "Let's benchmark for accuracy, latency, and resource utilization (Memory, GPU, and CPU).\n",
    "\n",
    "### Set up environment for evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58c48a70-637a-438a-aea9-0692e3ac422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load the pre-trained SAS model\n",
    "sas_model = SentenceTransformer('sentence-transformers/paraphrase-mpnet-base-v2')\n",
    "\n",
    "# Set the pad token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def compute_sas(predicted_answers, reference_answers):\n",
    "    \"\"\"\n",
    "    Compute the Semantic Answer Similarity (SAS) between a list of predicted answers and reference answers.\n",
    "    \n",
    "    Args:\n",
    "        predicted_answers (list of str): The list of predicted answer texts.\n",
    "        reference_answers (list of str): The list of reference answer texts.\n",
    "    \n",
    "    Returns:\n",
    "        float: The average SAS score between the predicted and reference answers.\n",
    "    \"\"\"\n",
    "    sas_scores = []\n",
    "    for predicted, reference in zip(predicted_answers, reference_answers):\n",
    "        predicted_embedding = sas_model.encode(predicted, convert_to_tensor=True)\n",
    "        reference_embedding = sas_model.encode(reference, convert_to_tensor=True)\n",
    "        sas_score = util.cos_sim(predicted_embedding, reference_embedding).item()\n",
    "        sas_scores.append(sas_score)\n",
    "    \n",
    "    return sum(sas_scores) / len(sas_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363df553-9982-41e6-893b-fb4c5a127263",
   "metadata": {},
   "source": [
    "### Benchmark for Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa5360df-f0cc-4435-9692-ea094c94349a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluate import load\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the evaluation metric and dataset\n",
    "metric = load(\"accuracy\")\n",
    "dataset = load_dataset(\"allenai/reward-bench\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "678444ea-8ad0-49aa-96db-f10933ce52f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions(examples):\n",
    "    predictions = []\n",
    "    \n",
    "    for prompt, chosen in zip(examples[\"prompt\"], examples[\"chosen\"]):\n",
    "        # Tokenize the input\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "        attention_mask = tokenizer(prompt, return_tensors=\"pt\").attention_mask.to(\"cuda\")\n",
    "        \n",
    "        # Generate the model's prediction\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            # max_new_tokens is more flexible than max_length,\n",
    "            # because it only caps the output\n",
    "            # so it won't fail if input is longer than the specified value\n",
    "            max_new_tokens=100,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        \n",
    "        # Decode the generated text\n",
    "        predicted_answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Append the predicted answer and reference answer to the predictions list\n",
    "        predictions.append({\"predicted\": predicted_answer, \"reference\": chosen})\n",
    "    \n",
    "    # Return the predictions\n",
    "    return {\"predictions\": predictions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f76c939-0330-4786-b620-16bf98a3728d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'What are the names of some famous actors that started their careers on Broadway?',\n",
       " 'chosen': 'Several famous actors started their careers on Broadway before making it big in film and television. Here are a few notable examples:\\n\\n1. Sarah Jessica Parker - Before she was Carrie Bradshaw on \"Sex and the City,\" Sarah Jessica Parker was a Broadway star, having appeared in productions like \"Annie\" as a child.\\n\\n2. Meryl Streep - Meryl Streep\\'s early career included Broadway productions such as \"Trelawny of the \\'Wells\\'\" and \"A Memory of Two Mondays / 27 Wagons Full of Cotton.\"\\n\\n3. Hugh Jackman - Hugh Jackman won a Tony Award for his role in \"The Boy from Oz\" and has been known for his stage work as well as his film career.\\n\\n4. Sutton Foster - Known for her television role in \"Younger,\" Sutton Foster is also a Broadway legend with leading roles in shows like \"Thoroughly Modern Millie\" and \"Anything Goes.\"\\n\\n5. Kristen Bell - Before she was the voice of Anna in \"Frozen\" or the star of \"The Good Place,\" Kristen Bell appeared in Broadway\\'s \"The Adventures of Tom Sawyer\" and \"The Crucible.\"\\n\\n6. Audra McDonald - Audra McDonald is a renowned Broadway actress with a record-breaking number of Tony Awards. She\\'s starred in \"Ragtime,\" \"Carousel,\" \"Master Class,\" and more.\\n\\n7. Nathan Lane - Nathan Lane is a Broadway veteran known for his roles in \"The Producers,\" \"A Funny Thing Happened on the Way to the Forum,\" and \"Angels in America.\"\\n\\n8. Idina Menzel - Before \"Frozen\" and \"Wicked\" made her a household name, Idina Menzel started on Broadway in shows like \"Rent\" and \"Hair.\"\\n\\n9. Lin-Manuel Miranda - Before \"Hamilton\" and \"In the Heights\" became huge hits, Lin-Manuel Miranda was performing on Broadway, eventually becoming a celebrated writer and actor.\\n\\n10. Lea Michele - Prior to her role on \"Glee,\" Lea Michele was a young Broadway actress in shows like \"Les Misérables,\" \"Ragtime,\" and \"Spring Awakening.\"\\n\\nThese actors are just a few examples of the many performers who have transitioned from the Broadway stage to broader fame in the entertainment industry. Broadway often serves as a proving ground for talent, and many actors continue to return to the stage throughout their careers.',\n",
       " 'chosen_model': 'GPT4-Turbo',\n",
       " 'rejected': 'Some famous actors that started their careers on Broadway include: Tom Hanks, Meryl Streep, Laurence Olivier, Christopher Walken, and Jeremy Irons.',\n",
       " 'rejected_model': 'alpaca-7b',\n",
       " 'subset': 'alpacaeval-easy',\n",
       " 'id': 0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what the dataset looks like.\n",
    "# Let's treat `prompt` like the Question and `chosen` like the Answer.\n",
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e6001d-e515-4d9d-9b55-9bb00eebbace",
   "metadata": {},
   "source": [
    "We sample a subset of the data, to more quickly get a rough idea if the evaluation works and awards at least some level of accuracy. From a model like Mistral, we should get more than 10% accuracy from our evaluation -- hopefully a lot more. If it is 0% accuracy, then either our evaluation code doesn't work at all, or the similarity checking is too rigid.\n",
    "\n",
    "Also, the dataset contains over 5000 records in the `train` portion alone. This can take hours to iterate through. Since we are not yet running this as a dedicated SageMaker job, a long job like this in a notebook can easily time out. As a solution, we are frequently saving the evaluation results as `evaluation_snapshots`, so we can pick up where we left off, if we get cut off.\n",
    "\n",
    "Initial results of the following job, on a subset of 80 examples (`num_samples = 80`), was a \"Semantic Answer Similarity\" of \"0.72\" -- meaning a 72% accuracy. This is satisfactory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fef66b77-d87a-413a-b5d1-edff4805e614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2cda56d26054c3c8d1b632daabb69a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 1/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9bd045f55924e048afa69e9accdf6ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 2/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31dc304c75734cb28ae37b111f8ed361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 3/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f78bb1d6f76143ddbaa482a9a85c010b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 4/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af275f087f75440f98ac0558d291bd34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 5/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fbf294e1bca43deb9118d5500d35244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 6/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdaf55a7484b4ec1ab1577b791437699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 7/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fffc18259096413886512f32e094cb8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 8/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d837e4a0ab1c47deb6f53622f41ce56c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 9/10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e2e35763fc041e1a1f9be5341e83fac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 10/10\n",
      "Semantic Answer Similarity: 0.72\n",
      "CPU times: user 5min 33s, sys: 2min 30s, total: 8min 3s\n",
      "Wall time: 7min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "\n",
    "batch_size=8\n",
    "\n",
    "results_dir = \"evaluation_snapshots\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "            \n",
    "subset_name = \"train\"\n",
    "subset_dataset = dataset[subset_name]\n",
    "\n",
    "# Limit to the first 80 Q&As\n",
    "num_samples = 80\n",
    "subset_dataset = subset_dataset.select(range(num_samples))\n",
    "\n",
    "num_batches = (len(subset_dataset) + batch_size - 1) // batch_size\n",
    "\n",
    "# Check for existing snapshots\n",
    "snapshot_files = sorted(os.listdir(results_dir))\n",
    "if snapshot_files:\n",
    "    latest_snapshot = snapshot_files[-1]\n",
    "    snapshot_path = os.path.join(results_dir, latest_snapshot)\n",
    "    snapshot = torch.load(snapshot_path)\n",
    "    predicted_answers = snapshot[\"predicted_answers\"]\n",
    "    reference_answers = snapshot[\"reference_answers\"]\n",
    "    start_batch_idx = int(latest_snapshot.split(\"_\")[-1].split(\".\")[0]) + 1\n",
    "else:\n",
    "    predicted_answers = []\n",
    "    reference_answers = []\n",
    "    start_batch_idx = 0\n",
    "\n",
    "for batch_idx in range(start_batch_idx, num_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min(start_idx + batch_size, len(subset_dataset))\n",
    "    batch_dataset = subset_dataset.select(range(start_idx, end_idx))\n",
    "    \n",
    "    batch_results = batch_dataset.map(generate_predictions, batched=True, batch_size=batch_size)\n",
    "    \n",
    "    batch_predicted_answers = [pred[\"predicted\"] for pred in batch_results[\"predictions\"]]\n",
    "    batch_reference_answers = [pred[\"reference\"] for pred in batch_results[\"predictions\"]]\n",
    "    \n",
    "    predicted_answers.extend(batch_predicted_answers)\n",
    "    reference_answers.extend(batch_reference_answers)\n",
    "    \n",
    "    # Save evaluation snapshot\n",
    "    snapshot_path = os.path.join(results_dir, f\"snapshot_{batch_idx}.pt\")\n",
    "    torch.save({\n",
    "        \"predicted_answers\": predicted_answers,\n",
    "        \"reference_answers\": reference_answers\n",
    "    }, snapshot_path)\n",
    "    \n",
    "    print(f\"Processed batch {batch_idx + 1}/{num_batches}\")\n",
    "\n",
    "# Compute SAS score\n",
    "sas_score = compute_sas(predicted_answers, reference_answers)\n",
    "print(f\"Semantic Answer Similarity: {sas_score:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82413a5c-e9b7-4fde-9e01-073be003f4f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
