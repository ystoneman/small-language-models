{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58d2ab92-e3a9-4adc-9a59-14030d02391c",
   "metadata": {},
   "source": [
    "# Small Language Model from LLM\n",
    "Download an LLM, prune and quantize it, and benchmark it each step of the way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d0bdf6-36ff-4dc3-acc5-d8197e941c1e",
   "metadata": {},
   "source": [
    "## Start by downloading an LLM\n",
    "I was going to use Llama 2 just because of how ubiquitous it currently is. However, I realized it requires HuggingFace authentication, because of how Meta AI has an approval process. To avoid cluttering the code with authentication, I just went with Mistral AI's Mistral model instead. We could choose larger versions of this model. However, to prove out and practice these model-optimization concepts, we can iterate faster with a smaller model like 7B.\n",
    "\n",
    "According to a discussion on HuggingFace, Llama-2 7B requires 28GB of GPU RAM. Assuming it is similar for Mistral 7B, and to be on the safe side, I'll over-provision with an ml.g5.4xlarge for my SageMaker Studio Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25d47b5-8cc1-4994-889d-c9ac5246d020",
   "metadata": {},
   "source": [
    "### Set up environment\n",
    "At first I got the error `KeyError: 'mistral'` when running `from_pretrained()`\n",
    "The solution was on [the model's HuggingFace page](https://huggingface.co/mistralai/Mistral-7B-v0.1#troubleshooting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e780782-bb76-4a90-b057-a892d69ae609",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d6c0cc-b577-42eb-a439-dfa0c25c1366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956cb49e-eff3-488d-8446-f21ac4e81baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_repo = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_repo)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_repo, torch_dtype=torch.float16).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce12127-8eea-460f-be5e-f7f6915c8a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple prompt\n",
    "prompt = \"Write a Haiku explaining biodynamic farming.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57296e79-63c8-4ea8-be79-88d724a4e075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d697f808-17c1-433f-b0d9-146e1fb0225a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate response\n",
    "output = model.generate(input_ids, max_length=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7a4ee2-53e3-485e-a9f3-68e4cb8c8f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode generated text\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True) \n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04d9d5c-e53e-471f-bec3-7fc3bbb84387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
